# Code transformers from scratch 
	## create self-attention abstract class to handle the attention calculations.s
